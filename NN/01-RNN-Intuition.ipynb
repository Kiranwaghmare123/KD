{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Intuition\n",
    "\n",
    "Credit: [Deep Learning A-Zâ„¢: Hands-On Artificial Neural Networks](https://www.udemy.com/deeplearning/learn/v4/content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan of attack**:\n",
    "- The idea behind Recurrent Neural Networks\n",
    "- The Vanishing Gradient Problem\n",
    "- Long Short-Term Memory (LSTM)\n",
    "- Practical Intuition\n",
    "- Extra: LSTM Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea behind Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole concept behind deep learning is to try to mimic the human brain.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-09-07 at 9.30.18 PM.png\">\n",
    "\n",
    "**ANN**\n",
    "- The main concept of ANN is the weights. Those weights represent long-term memory, which means once you run ANN and train it, you can switch it off and come back later. So the long-term memory weights make ANN similar to the **Temporal Lobe**.\n",
    "\n",
    "**CNN**\n",
    "- CNN is vision recognition of images and objects, so that is the **Ociipital Lobe**.\n",
    "\n",
    "**RNN**\n",
    "- RNNs are like short-term memory. They can remember things that just happened in the previous couple of observations and apply that knowledge going forward. It is similar to the ** Frontal Lobe**. Frontal Lobe is responsible for personality behavior or motor cortex working memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a simple Artificial Neural Network: 1 input, 1 output, and 1 hidden layer.\n",
    "    \n",
    "<img src=\"./images/Screen Shot 2017-09-07 at 9.32.40 PM.png\">\n",
    "\n",
    "We can represent the RNN by squashing the ANN. Think of it as we are looking the ANN from underneath - as a new dimension.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-09-07 at 9.34.48 PM.png\">\n",
    "\n",
    "To simplify things, we change multiple arrows to 2, then we twist the ANN to make it vertical.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-09-07 at 9.36.28 PM.png\">\n",
    "<img src=\"./images/Screen Shot 2017-09-07 at 9.36.35 PM.png\">\n",
    "\n",
    "We change the color of the hidden layer from green to blue and add another line, which represents the temporal loop. This means the hidden layer not only gives an output but also feeds back to itself.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-09-07 at 9.39.21 PM.png\">\n",
    "\n",
    "The official representation of RNN is to unroll the temporal loop, and put RNN in the following manner.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-09-07 at 9.41.16 PM.png\">\n",
    "\n",
    "Note that we are looking in the new dimension, so each one circle is not 1 neuron but actually a whole layer of neurons.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-09-07 at 9.42.10 PM.png\">\n",
    "\n",
    "This means we have inputs coming into the neurons then you have output but also the neurons connect to themselves through time. That is the whole concept when they have short-term memory that they remember what was in that neuron just previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Descent algorithm: we are trying to find the global minimum of the cost function, which is going to be the optimal solution for our neural network.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-09-08 at 9.24.11 AM.png\">\n",
    "\n",
    "- With Gradient Descent in an ANN, your information travels through the network to get the outputs, then the error is calculated and is propagated back to the network to update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Screen Shot 2017-09-08 at 9.29.51 AM.png\">\n",
    "- In a Recurrent Neural Network, you have information travels through the network and through time. Information from the previous time point keeps coming through the network.\n",
    "    - At each point in time, you can calculate the cost function of your error\n",
    "    \n",
    "<img src=\"./images/Screen Shot 2017-09-08 at 9.33.47 AM.png\">\n",
    "    \n",
    "- Let's focus on this one as time t to see what happens:\n",
    "    - You calculate the cost function $\\large \\epsilon_t$, then you need to propagate the cost function back to the network. Since you need to update the weights, so every single neuron which participated in the calculation of the output should have their weights updated.\n",
    "    - This means it is not only the neuron directly below $\\large \\epsilon_t$, but all neurons that contributed.     \n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-09-08 at 9.49.43 AM.png\">\n",
    "\n",
    "- Here we have $w_{rec}$, stands for weight recurring, that is the weight used to connect the hidden layers to themselves in the unrolled temporal loop. \n",
    "    - In simple term, we are multiplying the values by the weights from one layer to get to the next layer.\n",
    "    - This is where the problem is: when you multiple by something small, your value decreases quickly.\n",
    "    - Since weights are randomly assigned at the start of the neural network. If the initial weights are close to 0, since you are multiplying by multiple times, the more you multiple, the lower values you have. As you move backward, you gradient become less and less. That is the vanishing gradient problem\n",
    "    - The lower the gradient, the harder it is for the network to update the weights.\n",
    "    - The training becomes a vicious cycle, because of the gradients are so small, the training is slow and the outputs from the hidden layers are incorrect. Therefore, you are training on the non-final outputs, since the weights are not being trained properly, the whole network is not being trained properly.\n",
    "\n",
    "- In summary\n",
    "    - If $w_{rec}$ is small, you have the vanishing gradient problem\n",
    "    - If $w_{rec}$ is large, you have the exploding gradient problem\n",
    "\n",
    "**Solutions**:\n",
    "\n",
    "1. Exploding Gradient\n",
    "    - Truncated Backpropagation: stop back-propagating after a certain point\n",
    "    - Penalties: the gradient is being penalized and is artifically reduced.\n",
    "    - Gradient Clipping: maximum limit for the gradient\n",
    "2. Vanishing Gradient\n",
    "    - Weight Initilization: initialize the weights to minimize the potential for vanishing gradient.\n",
    "    - Echo State Networks\n",
    "    - Long Short-Term Memory Networks (LSTMs): very popular.\n",
    "    \n",
    "**Additional Reading**\n",
    "* Yoshua Bengio, 1994, [Learning Long-Term Dependencies with Gradient Descent is Difficult](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)\n",
    "* Razvan Pascanu, 2013, [On the difficulty of training recurrent neural networks](http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short-Term Memory Networks - LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's simplify the problem of RNNs:\n",
    "    - If $w_{rec}$ < 1, you have the vanishing gradient problem\n",
    "    - If $w_{rec}$ > 1, you have the exploding gradient problem\n",
    "    - The simplest solution is to make the weight recurring $w_{rec}$ = 1, that is the idea of LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./images/Screen Shot 2018-11-07 at 9.47.22 PM.png\">\n",
    "\n",
    "- LSTM has a memory cell, which is the upper straight line at the top, that goes through time, sometimes it is removed or added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Screen Shot 2018-11-07 at 10.05.03 PM.png\">\n",
    "- Notation:\n",
    "    - x: input\n",
    "    - C: memory cell\n",
    "    - h: output\n",
    "- An LSTM module takes in 3 input and has 2 outputs (2 $h_t$ is the same). Note that all input, output and memory cell are all vectors\n",
    "    - Vector transfer: The arrow line represent all vector being transferred\n",
    "    - Concatenate: 2 pipes running in parallel\n",
    "    - Copy: The memories are being copied\n",
    "    - Pointwise operation: \n",
    "        - $\\bigotimes$: valves, 3 valves are forget valve, memory valve, and output valve respectively. Controlled by the sigmoid function.\n",
    "            - think of it like a water valves. If it opens, memory flows freely, if it closed, memory is cut off.\n",
    "        - $\\bigoplus$: T-shape joint, additional memory is added.\n",
    "        - $tanh$ operation: makes output value between -1 and 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "62px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
