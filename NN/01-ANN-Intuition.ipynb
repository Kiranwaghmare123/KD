{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#ANN-Intuition\" data-toc-modified-id=\"ANN-Intuition-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ANN Intuition</a></div><div class=\"lev2 toc-item\"><a href=\"#The-Neuron\" data-toc-modified-id=\"The-Neuron-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>The Neuron</a></div><div class=\"lev2 toc-item\"><a href=\"#The-Activation-Function\" data-toc-modified-id=\"The-Activation-Function-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>The Activation Function</a></div><div class=\"lev2 toc-item\"><a href=\"#How-do-NNs-work?\" data-toc-modified-id=\"How-do-NNs-work?-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>How do NNs work?</a></div><div class=\"lev2 toc-item\"><a href=\"#How-do-Neural-Networks-learn?\" data-toc-modified-id=\"How-do-Neural-Networks-learn?-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>How do Neural Networks learn?</a></div><div class=\"lev2 toc-item\"><a href=\"#Gradient-Descent\" data-toc-modified-id=\"Gradient-Descent-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Gradient Descent</a></div><div class=\"lev2 toc-item\"><a href=\"#Stochastic-Gradient-Descent\" data-toc-modified-id=\"Stochastic-Gradient-Descent-16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Stochastic Gradient Descent</a></div><div class=\"lev2 toc-item\"><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-17\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Backpropagation</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Intuition\n",
    "\n",
    "Credit: [Deep Learning A-Zâ„¢: Hands-On Artificial Neural Networks](https://www.udemy.com/deeplearning/learn/v4/content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Screen Shot 2017-06-11 at 10.26.10 PM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The Neuron__ contains:\n",
    "    - Neuron\n",
    "    - Dendrites: receiver of the neuron\n",
    "    - Axon: transmitter of the signal for the neuron\n",
    "    \n",
    "How can we represent neuron in machine?\n",
    "- Input signal: Dendrites\n",
    "- Output signal: Axon\n",
    "- Input values, their signals pass through Synapse to Neuron, then the neuron has an output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Screen Shot 2017-06-11 at 10.39.38 PM.png\">\n",
    "\n",
    "- Input layer:\n",
    "    - Receive all input values (independent variables). These independent variables are all for 1 single observation (1 row of all values: age, bank amount, ...)\n",
    "    - You should standardize these input values.\n",
    "    \n",
    "- Output value: can be\n",
    "    - Continous (price)\n",
    "    - Binary\n",
    "    - Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Screen Shot 2017-06-11 at 10.46.10 PM.png\">\n",
    "\n",
    "Weights: are how neural networks learn. \n",
    "- By adjusting the weights, the neural network decides for every single case which signal is important, and which one is not.\n",
    "- Weights are adjusted through the process of learning.\n",
    "\n",
    "What happen inside the Neuron?\n",
    "- 1st step: $\\sum_{i=1}^m w_i x_i$, takes the weighted sum of all the input values.\n",
    "- 2nd step: apply the activation function ($\\phi$) to the weighted sum.\n",
    "- 3rd step: the neuron passes the signal to the next neuron down the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more activation fucntions, but we are going to look at 4 different types of activation function.\n",
    "\n",
    "__Threshold function__\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-06-11 at 9.59.29 PM.png\">\n",
    "\n",
    "- If the value is less than 0, then the Threshold function passes on 0.\n",
    "- If the value is more than or equal to 0, then the Threshold function passes on 1.\n",
    "- basically yes/no type of function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoid function__\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-06-11 at 10.00.37 PM.png\">\n",
    "\n",
    "- a smooth function\n",
    "- very useful in the output layer when we try to predict probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Rectifier__\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-06-11 at 10.06.12 PM.png\">\n",
    "\n",
    "- mostly used, very popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hyperbolic Tangent (tanh)__\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-06-11 at 10.07.22 PM.png\">\n",
    "\n",
    "- similar to sigmoid function, but goes below 0 to -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Assuming the dependent variable is binary (y=0 or 1), which activation function can we use? We have 2 options:\n",
    "    - Threshold function: It fits perfect when we need 0 or 1\n",
    "    - Sigmoid function: between 0 and 1, gives us the probability of 0 and 1.\n",
    "<img src=\"./images/Screen Shot 2017-06-11 at 10.12.41 PM.png\">\n",
    "\n",
    "- We have a very common combination where:\n",
    "    - Hidden layer: use rectifier function\n",
    "    - Output layer: use sigmoid function to give us the probabilities\n",
    "<img src=\"./images/Screen Shot 2017-06-11 at 10.59.53 PM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do NNs work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Basic form of a neural network\n",
    "- Only contains an input layer and an output layer.\n",
    "- All of the input variables will be weighted up by the synapse and the price will be calculated by the weighted sum of the all inputs. You can use any activation functions to get a certain output.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-06-15 at 3.29.00 PM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, NN has an extra advantage that increases the accuracy which is hidden layers.\n",
    "\n",
    "For each neuron in the hidden layers:\n",
    "    - The weights of each input variables are not equal. Some weights may have non-zero values, some weights may have zero values. Because not all inputs are important for that neuron. For example: the first neuron only care about 2 inputs: area and distance from city, we can explain that the further from city, the area of the property is larger. That is why we don't need to draw the line of the synapses which are not important.\n",
    "    \n",
    "- Each one of the neuron cannot predict the price, but together they can do a proper job.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-06-15 at 3.48.32 PM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do Neural Networks learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perceptron: a single layer feed forward neural network\n",
    "- Output value ($\\hat y$): predicted value by the neural network\n",
    "- Actual value: y\n",
    "- We will calculate the cost function, which is the difference (error) between predicted value and actual value:\n",
    "    - Cost function: $\\frac{1}{2}(\\hat y - y)^2$\n",
    "    - Our goal is to minimize the cost fuction\n",
    "- After having the cost function, we will feed the information back to the neural network, then the weights get updated to minimize the cost function.\n",
    "- 1 epoch: is when we go through the whole dataset.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-06-17 at 9.24.54 AM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are many cost functions. Here is [A list of cost functions used in neural networks, alongside applications](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How can we minimize the cost function?\n",
    "- 1 approach is the brute force approach, when we try out lots of weights, then we have this graph:\n",
    "    - y-axis: cost function\n",
    "    - x-axis: $\\hat y$\n",
    "    \n",
    "<img src=\"./images/Screen Shot 2017-06-17 at 10.59.44 AM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we should not use this brute-force approach by trying out lots of parameters and inputs for weights?\n",
    "- Because as you increase number of weights or synapses, you have to face the __curse of dimensionality__.\n",
    "- Example to understand the __curse of dimensionality__:\n",
    "    - We have a 1 layer neural network of 25 weights. If we need to try out 1000 combination of weights. We need: $1000^{25}= 10^{75}$ combinations.\n",
    "    - Sunway Taihulight: World's fastest super computer can do 93 PFLOPS (93 x $10^{15}$ floating operation/ second). It will take $10^{75} / 93 x 10^{15} = 1.08 * 10^{58} \\text{seconds} = 3.42 * 10^{50} \\text{years}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a different approach: __Gradient Descent__\n",
    "\n",
    "__Intuition__:\n",
    "- Let's say we have a starting cost function. By looking at the angle of the cost function, we just need to differentiate, find out the slope is positive or negative, then we can decide to go downhill or uphill until we reach the minimum.\n",
    "- It is called gradient descent, because you are descending into the minimum of the cost function.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-06-17 at 11.22.36 AM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Descent__ requires the cost function to be a convex function. If our cost function is not convex, then our gradient descent will lead us to the local minimum instead of the global minimum.\n",
    "\n",
    "<img src=\"./images/Screen Shot 2017-06-17 at 11.39.26 AM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, __Stochastic Gradient Descent__ does not require our cost function to be convex. \n",
    "\n",
    "Differences between Gradient Descent (also called Batch Gradient Descent) and Stochastic Gradient Descent\n",
    "\n",
    "| __Gradient Descent__  | __Stochastic Gradient Descent__  |\n",
    "|:-:|:-:|\n",
    "| Calculate cost functions and adjust weights by taking all input values | Calculate the cost function and adjust the weight by looking at 1 row at a time |\n",
    "| runs lower  | runs faster  |\n",
    "| deterministic algorithm  | stochatic algorithm (random) |\n",
    "\n",
    "The reason why Stochastic Gradient Descent helps avoid the problem of stucking in local minimum is because Stochastic Gradient Descent has a much higher flunctuation. It is much more higher to find global minimum.\n",
    "\n",
    "__Additional Reading__:\n",
    "- [A Neural Network in 13 lines of Python (Part 2 - Gradient Descent)](https://iamtrask.github.io/2015/07/27/python-network-part2/)\n",
    "- [Chapter 2 - How the backpropagation algorithm works](http://neuralnetworksanddeeplearning.com/chap2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Forward Propagation__: Information is entered into the input layer and then it is propagated forward to get the output value $\\hat y$. The output values are then compared to actual values to compute errors. The errors are then back-propagated through the network in the opposite direction to train the network by adjusting the weights.\n",
    "\n",
    "Backpropagation allows us to adjust all the weights at the same time.\n",
    "\n",
    "__Additional Reading__: [Chapter 2 - How the backpropagation algorithm works](http://neuralnetworksanddeeplearning.com/chap2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Steps-by-steps walkthough in the training of ANN__\n",
    "    \n",
    "<img src=\"./images/Screen Shot 2017-06-17 at 12.04.33 PM.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_number_sections": false,
   "toc_section_display": "block",
   "toc_threshold": "8",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
