{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a64fe2",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10aa7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ed344",
   "metadata": {},
   "source": [
    "# 1.Tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "094acb36",
   "metadata": {},
   "source": [
    "Tokenization is the process of dividing the whole text into tokens.\n",
    "\n",
    "It is mainly of two types:\n",
    "\n",
    "1.Word Tokenization (separated by words)\n",
    "2.Sentence Tokenizer ( separated by sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5d3448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c2c9bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b5987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "text=\"Hello there, how are you doing today? The weather is great today. The sky is blue. Python is awesome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8c866e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there, how are you doing today?', 'The weather is great today.', 'The sky is blue.', 'Python is awesome']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "847e8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'today', '.', 'The', 'sky', 'is', 'blue', '.', 'Python', 'is', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca70cf2",
   "metadata": {},
   "source": [
    "# 2. Stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "647e1253",
   "metadata": {},
   "source": [
    "In general stopwords are the words in any language which does not add much meaning to a sentence. In NLP stopwords are those words which are not important in analyzing the data.\n",
    "\n",
    "Example : he,she,hi,and etc.\n",
    "Our main task is to remove all the stopwords for the text to do any further processing.\n",
    "\n",
    "There are a total of 179 stopwords in English, using NLTK we can see all the stopwords in English.\n",
    "\n",
    "We Just need to import stopwords from the library nltk.corpus ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21238674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8312bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To remove Stopwords for a particular text\n",
    "from nltk.corpus import stopwords\n",
    "text='he is a good boy. he is very good in coding'\n",
    "text=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_no_stopwords=[word for word in text if word not in stopwords.words('english')]\n",
    "text_with_no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63394ac0",
   "metadata": {},
   "source": [
    "# 3.Stemming"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff83ef7d",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.\n",
    "\n",
    "In simple words, we can say that stemming is the process of removing plural and adjectives from the word.\n",
    "\n",
    "Example :\n",
    "loved → love, learning →learn\n",
    "\n",
    "In python, we can implement stemming by using PorterStemmer . we can import it from the library nltk.stem ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "#Creating an object for porterstemmer\n",
    "ps=PorterStemmer()\n",
    "\n",
    "#Example words\n",
    "example_word = ['earn', \"earning\", \"earned\", \"earns\"]\n",
    "\n",
    "for w in example_word:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddf4ba",
   "metadata": {},
   "source": [
    "# 4.Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191612cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "139962d8",
   "metadata": {},
   "source": [
    "# Lemmatization usually refers to doing things properly with the use of vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "In simple words lemmatization does the same work as stemming, the difference is that lemmatization returns a meaningful word.\n",
    "\n",
    "Example:\n",
    "\n",
    "Stemming\n",
    "history → histori\n",
    "Lemmatizing\n",
    "history → history\n",
    "\n",
    "It is Mostly used when designing chatbots, Q&A bots, text prediction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6eeb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "example_word=['history','formality','changes']\n",
    "for w in example_word:\n",
    "    print(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beedecf3",
   "metadata": {},
   "source": [
    "# 5. WordNet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf47010d",
   "metadata": {},
   "source": [
    "WordNet is the lexical database i.e. dictionary for the English language, specifically designed for natural language processing.\n",
    "We can use wordnet for finding synonyms and antonyms.\n",
    "\n",
    "In python, we can import wordnet from nltk.corpus .\n",
    "Code For Finding Synonym and antonym for a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"happy\"):\n",
    "    for i in syn.lemmas():\n",
    "        synonyms.append(i.name())\n",
    "        if i.antonyms():\n",
    "            antonyms.append(i.antonyms()[0].name())\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45add08d",
   "metadata": {},
   "source": [
    "# 6. Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8d317bc",
   "metadata": {},
   "source": [
    "It is a process of converting a sentence to forms — a list of words, a list of tuples (where each tuple is having a form (word, tag)). The tag in the case is a part-of-speech tag and signifies whether the word is a noun, adjective, verb, and so on."
   ]
  },
  {
   "cell_type": "raw",
   "id": "afd6fc90",
   "metadata": {},
   "source": [
    "###Part of Speech Tag List\n",
    "\n",
    "CC coordinating conjunction\n",
    " CD cardinal digit\n",
    " DT determiner\n",
    " EX existential there (like: “there is” … think of it like “there”)\n",
    " FW foreign word\n",
    " IN preposition/subordinating conjunction\n",
    " JJ adjective ‘big’\n",
    " JJR adjective, comparative ‘bigger’\n",
    " JJS adjective, superlative ‘biggest’\n",
    " LS list marker 1)\n",
    " MD modal could, will\n",
    " NN noun, singular ‘desk’\n",
    " NNS noun plural ‘desks’\n",
    " NNP proper noun, singular ‘Harrison’\n",
    " NNPS proper noun, plural ‘Americans’\n",
    " PDT predeterminer ‘all the kids’\n",
    " POS possessive ending parent’s\n",
    " PRP personal pronoun I, he, she\n",
    " PRP possessive pronoun my, his, hers\n",
    " RB adverb very, silently,\n",
    " RBR adverb, comparative better\n",
    " RBS adverb, superlative best\n",
    " RP particle give up\n",
    " TO to go ‘to’ the store.\n",
    " UH interjection errrrrrrrm\n",
    " VB verb, base form take\n",
    " VBD verb, past tense took\n",
    " VBG verb, gerund/present participle taking\n",
    " VBN verb, past participle taken\n",
    " VBP verb, sing. present, non-3d take\n",
    " VBZ verb, 3rd person sing. present takes\n",
    " WDT wh-determiner which\n",
    " WP wh-pronoun who, what\n",
    " WP possessive wh-pronoun whose\n",
    " WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '''\n",
    "An sincerity so extremity he additions. Her yet there truth merit. Mrs all projecting favourable now unpleasing. Son law garden chatty temper. Oh children provided to mr elegance marriage strongly. Off can admiration prosperous now devonshire diminution law.\n",
    "'''\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b81494",
   "metadata": {},
   "source": [
    "# 7. Bag of words"
   ]
  },
  {
   "cell_type": "raw",
   "id": "694787b0",
   "metadata": {},
   "source": [
    "Till now we have understood about tokenizing, stemming, and lemmatizing. all of these are the part of the text cleaning, now after cleaning the text we need to convert the text into some kind of numerical representation called vectors so that we can feed the data to a machine learning model for further processing.\n",
    "\n",
    "For converting the data into vectors we make use of some predefined libraries in python."
   ]
  },
  {
   "cell_type": "raw",
   "id": "50815447",
   "metadata": {},
   "source": [
    "sent1 = he is a good boy\n",
    "sent2 = she is a good girl\n",
    "sent3 = boy and girl are good \n",
    "        |\n",
    "        |\n",
    "  After removal of stopwords , lematization or stemming\n",
    "sent1 = good boy\n",
    "sent2 = good girl\n",
    "sent3 = boy girl good  \n",
    "        | ### Now we will calculate the frequency for each word by\n",
    "        |     calculating the occurrence of each word\n",
    "word  frequency\n",
    "good     3\n",
    "boy      2\n",
    "girl     2\n",
    "         | ## Then according to their occurrence we assign o or 1 \n",
    "         |    according to their occurrence in the sentence\n",
    "         | ## 1 for present and 0 fot not present\n",
    "         f1  f2   f3\n",
    "        girl good boy   \n",
    "sent1    0    1    1     \n",
    "sent2    1    0    1\n",
    "sent3    1    1    1\n",
    "\n",
    "###After this we pass the vector form to machine learning model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4d6fced",
   "metadata": {},
   "source": [
    "The above process can be done using a CountVectorizer in python, we can import the same from sklearn.feature_extraction.text .\n",
    "\n",
    "CODE to implement CountVectorizer In python\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sent = pd.DataFrame(['he is a good boy', 'she is a good girl', 'boy and girl are good'],columns=['text'])\n",
    "corpus = []\n",
    "for i in range(0,3):\n",
    "    words = sent['text'][i]\n",
    "    words  = word_tokenize(words)\n",
    "    texts = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    text = ' '.join(texts)\n",
    "    corpus.append(text)\n",
    "print(corpus)   #### Cleaned Data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer() ## Creating Object for CountVectorizer\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X  ## Vectorize Form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09c762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
